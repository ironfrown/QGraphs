{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "92426396-bb03-451f-89db-5f44073ebaeb",
   "metadata": {},
   "source": [
    "# QGraphs Data - Versions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4189476b-6222-498d-bd9a-56b8e28c59f7",
   "metadata": {},
   "source": [
    "*This collection of notebooks explores creation and use of* ***QGgraphs (Quantum Digraphs)*** *in* ***Networkx+PennyLane+PyTorch**.*\n",
    "\n",
    "**By:** Jacob Cybulski<br>\n",
    "**Date:** August 2024<br>\n",
    "**Aims:** The goal of this notebook is to test some *Quantum Digraph (QGraphs)* features and functions.<br/>\n",
    "**Refs:**\n",
    "- Brownlee, J., 2019. <a href=\"https://machinelearningmastery.com/divergence-between-probability-distributions/\" target=\"_blank\">How to Calculate the KL Divergence for Machine Learning</a>. MachineLearningMastery.com.\n",
    "- <a href=\"https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence\" target=\"_blank\">Kullbackâ€“Leibler divergence</a>, Wikipedia.\n",
    "- <a href=\"https://en.wikipedia.org/wiki/Jensen%E2%80%93Shannon_divergence\" target=\"_blank\">Jensen-Shannon Divergence</a>, Wikipedia.\n",
    "- <a href=\"https://pytorch.org/docs/stable/generated/torch.nn.KLDivLoss.html\" target=\"_blank\">KLDivLoss</a>. PyTorch docs.\n",
    "\n",
    "**PennyLane technical refs:**\n",
    "- https://pennylane.ai/codebook/\n",
    "- https://docs.pennylane.ai/en/stable/index.html\n",
    "- https://docs.pennylane.ai/en/stable/introduction/interfaces.html (see refs to Fourier and fun fitting examples below)\n",
    "- https://docs.pennylane.ai/en/stable/code/api/pennylane.qnn.TorchLayer.html (see ref to an example below)\n",
    "- https://pytorch.org/docs/stable/optim.html\n",
    "- Example of circuit building: https://pennylane.ai/qml/demos/tutorial_learning_few_data/\n",
    "- Example of circuit building: https://pennylane.ai/qml/demos/tutorial_expressivity_fourier_series/\n",
    "- Example of training: https://pennylane.ai/qml/demos/function_fitting_qsp/\n",
    "- Example of creating Torch nn layer: https://pennylane.ai/qml/demos/tutorial_qnn_module_torch/\n",
    "\n",
    "**PyTorch technical refs:**\n",
    "- Initialise weights: https://stackoverflow.com/questions/49433936/how-do-i-initialize-weights-in-pytorch\n",
    "- First NN in PyTorch: https://machinelearningmastery.com/develop-your-first-neural-network-with-pytorch-step-by-step/\n",
    "- PyTorch training loop: https://machinelearningmastery.com/creating-a-training-loop-for-pytorch-models/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d579328e-0ecb-4a7a-8d9f-80caf84500a2",
   "metadata": {},
   "source": [
    "## What are QGraphs\n",
    "\n",
    "A *QGraph* is a weighted directed graph, which excludes parallel edges (in the same direction), but allowing loops. It defines a stochastic navigation between its verteces, where the probability of taking a particular edge from a given vertex is given by the edge weight.\n",
    "\n",
    "Formally, a qgraph $G$ is defined as a weighted digraph:\n",
    "\n",
    "$$G = \\{V, E, w\\},$$\n",
    "\n",
    "where $V = \\{v\\}$ is a set of digraph vertices, $E = \\{(v_s,v_t) : v_s, v_t \\in V\\}$ is a set of directional edges, and $|V|$ being the number of vertices in $V$.<br/>\n",
    "Function $w(V, V)\\rightarrow R$ assigns weights to all edges, such that:\n",
    "\n",
    "$$w(v_s, v_t) =\n",
    "\\left\\{\n",
    "\t\\begin{array}{ll}\n",
    "\t\tp(v_t | v_s)  & \\forall_{v_s, v_t \\in V,\\;(v_s, v_t) \\in E}\\\\\n",
    "\t\t0 & \\forall_{v_s, v_t \\in V,\\;(v_s, v_t) \\notin E}\n",
    "\t\\end{array}\n",
    "\\right.\n",
    "$$\n",
    "\n",
    "where $p(v_t | v_s)$ is the probability of navigating over the edge $(v_s, v_t)$ when arrived at the source vertex $v_s$ to its target $v_t$ (including the self), and:\n",
    "\n",
    "$$\\forall_{(v_s, v_t) \\in E}\\: \\sum_{v_t} w(v_s, v_t) = 1.$$\n",
    "\n",
    "This means that every digraph vertex must have at least one outgoing link.\n",
    "\n",
    "An arbitrary weighted digraph $G = \\{V, E, w(V, V)\\}$ can be expanded to become a QGraph $G^{'} = \\{V^{'}, E^{'}, w^{'}(V^{'}, V^{'})\\}$ by adding loops to all verteces $v$ with outdegree 0, and rescaling weights to add up to 1. Now:\n",
    "\n",
    "$$\\forall_{v \\in V}\\; v \\in V^{'} \\land \\forall_{e \\in E}\\; e \\in E^{'}$$\n",
    "$$\\forall_{v_s \\in V}\\;(\\forall_{v_t \\in V}\\;(v_s, v_t) \\notin E) \\Longrightarrow (v_s, v_s) \\in E^{'} \\land w^{'}(v_s, v_s) = 1$$\n",
    "$$\\forall_{(v_s, v_t) \\in E}\\; w^{'}(v_s, v_t) = \\frac{w(v_s, v_t)}{\\sum_{(v_s, v_i) \\in E}\\; w(v_s, v_i)}$$\n",
    "\n",
    "## PennyLane Implementation\n",
    "\n",
    "A QGraph $G = \\{V, E, w\\}$, with $v \\in V$ as the vertex unique numeric identifier, represents a parameterised function $f_\\theta(V) \\rightarrow V$,\n",
    "which can be realised as a quantum circuit $U(v, \\theta)$ measured projectively with observable $\\mathcal{M}$:\n",
    "\n",
    "$$\n",
    "\\begin{gather}\n",
    "    f_\\theta(v) = tr[ \\mathcal{M} U(v, \\theta)^\\dagger \\vert 0 \\rangle \\langle 0 \\vert U(v, \\theta) ],\\quad and\\\\\n",
    "    U(v, \\theta) = A(\\theta)S(v).\n",
    "\\end{gather}\n",
    "$$\n",
    "\n",
    "$S(v)$ is a feature map and $A(\\theta)$ an ansatz with trainable parameters $\\theta = (\\theta_0, \\theta_1, ..., \\theta_n)$.\n",
    "The ansatz can be trained on the graph edges $e = (v_s, v_t) \\in E$ and their weights $w(v_s, v_t)$, such that $v_s$ would be passed as the input to $U(v, \\theta)$, which is subsequently executed repeatedly to produce a distribution of possible outcomes $v_t$, each approximating the probability $p(v_t|v_s)$ aimed to approach the value of its edge weight $w(v_s, v_t)$.\n",
    "\n",
    "The circuit $U(v, \\theta)$ training can be facilitated by running $U(v_s, \\theta)$ repeatedly for each vertex $v_s$ and measuring the difference between two probability distributions of possible paths from vertex $v_s$, i.e. the observed probability distribution $P(v_s) = \\{p(v_t | v_s) : \\forall_{v_t \\in V}\\}$ and the expected distribution $W(v_s) = \\{w(v_s, v_t) : \\forall_{e=(v_s, v_t) \\in E}\\}$. \n",
    "The standard loss function that could support the model training is $L1$ or $L2$, which would measure the distance between pairs of probability distributions $W(v_s)$ and $P(v_s)$. As the size of $P(v_s)$ distribution cannot be predicted in advance, hence the computational complexity of the $L1(W(v_s), P(v_s))$ measurement would be $O({\\vert V \\vert}^2)$.\n",
    "The entire cost would then be measured as $MAE(W, P)$ or $MSE(W, P)$ with the complexity of $O({\\vert V \\vert}^3)$.\n",
    "\n",
    "An alternative loss function is *Kullback-Leibler Divergence*, stated as a function $KL(W(v_s) || P(v_s))$ representing the magnitude of our \"surprise\" when the model adopts the observed distribution $P(v_s)$ instead of the actual distribution $W(v_s)$. The $KL$ loss function for the vertex $v_s$ can be defined as follows (adapted from Brownlee 2019):\n",
    "\n",
    "$$KL(W(v_s) || P(v_s)) = \\sum_{(v_s, v_t) \\in E} w(v_s, v_t) * log(w(v_s, v_t) / p(v_t | v_s))$$\n",
    "\n",
    "Therefore, the cost (average loss) of differences between observed and expected navigations for all digraph vertices is:\n",
    "\n",
    "$$\n",
    "\\begin{array}{ll}\n",
    "    KL(W || P) \n",
    "    &=&\\frac{1}{|V|}\\sum_{v_s \\in V}\\;KL(W(v_s) || P(v_s))\\\\\n",
    "    &=&\\frac{1}{|V|}\\sum_{v_s \\in V}\\sum_{(v_s, v_t) \\in E}\\; w(v_s, v_t) * log(w(v_s, v_t) / p(v_t | v_s)) \\\\\n",
    "    &=&\\frac{1}{|V|}\\sum_{(v_s, v_t) \\in E}\\; w(v_s, v_t) * log(w(v_s, v_t) / p(v_t | v_s))\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "For the $KL$ function to be computable, we only consider pairs of vertices $(v_s, v_t)$ that have non-zero weights $w(v_s, v_t)$ and non-zero probability $p(v_t | v_s)$. \n",
    "Also note that $KL$ ignores those vertex pairs which emerge as \"probably navigable\" during model training, but not actually connected by an edge, meaning their pairwise weight is zero. \n",
    "This leads to the more selective cost calculation, with its complexity to be $O(|V||E|)$, with the upper bound of $O(|V|^3)$, however significantly lower for sparse graphs.\n",
    "\n",
    "*Kullback-Leibler Divergence* is not symmetrical, i.e. $KL(W || P) \\neq KL(P || W)$, and tends to generate very large negative values. Therefore, a related *Jensen-Shannon Divergence* could be used as a replacement. $JS$ loss function can be defined as follows (adapted from Brownlee 2019):\n",
    "\n",
    "$$\n",
    "\\begin{array}{ll}\n",
    "    &&\\text{the loss function is:}\\\\\n",
    "    \\\\\n",
    "    &&for\\; M(v_s) = 0.5\\; (W(v_s) + P(v_s)),\\\\\n",
    "    JS(W(v_s) || P(v_s)) \n",
    "    &=&0.5 * KL(W(v_s) || M(v_s)) + 0.5 * KL(P(v_s) || M(v_s)),\\\\\n",
    "    \\\\\n",
    "    &&\\text{and the cost (average loss) is:}\\\\\n",
    "    \\\\\n",
    "    &&for\\; M = 0.5\\; (W + P),\\\\\n",
    "    JS(W || P) \n",
    "    &=&\\frac{1}{|V|}\\sum_{v_s \\in V}\\;JS(W(v_s) || P(v_s))\\\\\n",
    "    &=&0.5 * \\frac{1}{|V|}\\sum_{v_s \\in V}\\;(KL(W(v_s) || M(v_s)) + KL(P(v_s) || M(v_s))) \\\\\n",
    "    &=&0.5 * (\\frac{1}{|V|}\\sum_{v_s \\in V}\\;KL(W(v_s) || M(v_s)) + \\frac{1}{|V|}\\sum_{v_s \\in V}\\;KL(P(v_s) || M(v_s))) \\\\\n",
    "    &=&0.5 * (KL(W || M) + KL(P || M))\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "As noted by Brownlee (2019), when compared with KL divergence, $JS(W(v_s) || P(v_s))$, and consequently $JS(W || P)$, is a more useful measure for model training - it is smooth and normalized, and, when using the base-2 logarithm, its scores are in the range from 0 (identical) to 1 (maximally different). Its computational complexity is also $O(|V||E|)$.\n",
    "\n",
    "Note that in PyTorch, the class <a href=\"https://pytorch.org/docs/stable/generated/torch.nn.KLDivLoss.html\" target=\"_blank\">KLDivLoss</a> (see examples) defines the KL function, which takes as its arguments *input* (P), which is the result produced by a model (e.g. NN), and a *target* (W) which defines observations drawn from the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2ce99e6-c25f-48a6-bafb-77edf19bc4cb",
   "metadata": {},
   "source": [
    "## Versions\n",
    "All notebooks will keep consistent version numbers\n",
    "\n",
    "*pl_qgraphs_main:* Commenced work on *QGraphs (QG)*\n",
    "- **V1_00 (240808)** Initial graph representation in *networkx*<br/>\n",
    "  Added an extra vertex for redirection from stop vertices\n",
    "- **V1_01 (240814)** Added mathematical formulation\n",
    "- **V1_02 (240816)** Changed representation and maths<br/>\n",
    "  Added loops for redirection from stop vertices\n",
    "- **V1_03 (240824)** Initial work on graph training in *PennyLane*\n",
    "- **V1_04 (240828)** Added KL and JSD loss/cost functions\n",
    "- **V1_05 (240901)** Notebook split into data and training parts<br/>\n",
    "  As the code was getting very long, it was split into data prep and model training\n",
    "- **V1_14 (241228)** Created main versioned after training component<br/>\n",
    "  All main description moved into this \"main\" notebook\n",
    "\n",
    "*pl_qgraphs_data:* Digraph generation\n",
    "- **V1_05 (240901)** Generation of an 8 vertices graphs\n",
    "- **V1_06 (240909)** Generation of a 16 vertices graphs\n",
    "- **V1_07 (240909)** Change of the log file naming conventions\n",
    "- **V1_10 (240911)** utils.Digraph.py plots save eps image, logs are created to save data\n",
    "- **V1_14 (241228)** Cleanup\n",
    "\n",
    "*pl_qgraphs_train:* QGraph model creation and training\n",
    "- **V1_05 (240901)** Added QG model creation, started training\n",
    "- **V1_06 (240908)** Need to recode QG input, from a single n to its binary representation\n",
    "- **V1_07 (240909)** Fixed classical and hybrid modes, improved performance, esp. with JSD\n",
    "- **V1_08 (240909)** Larger graph tested, code slightly reorganised\n",
    "- **V1_09 (240909)** Testing different modes: quantum, classic and hybrid\n",
    "- **V1_10 (240911)** Fixed classic model with JSD by adding softmax\n",
    "- **V1_11 (240912)** Included side-by-side comparison of the expected vs observed weight distributions\n",
    "- **V1_12 (241219)** Added several performance scores\n",
    "- **V1_13 (241219)** Logs are incorporated to save graph definitions and performance data\n",
    "- **V1_14 (241228)** QGraphs are now being consistently normalised + training made independent from analysis notebook\n",
    "\n",
    "*pl_qgraphs_analysis:* QGraph model analysis\n",
    "- **V1_14 (241228)** Analysis made independent from training notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "403bec1a-13a8-40e8-bb51-c7650afc4e44",
   "metadata": {},
   "source": [
    "## System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d7dab2a6-0f2a-4ffd-a23d-dde0c3d6297c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment:\n",
      "\n",
      "/home/jacob/miniconda3/envs/pl-cuda12\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(f'Environment:\\n\\n{sys.prefix}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bdd995cd-9bec-4d8d-b1ea-de0634b026e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "networkx                  3.4.2\n",
      "PennyLane                 0.39.0\n",
      "PennyLane_Lightning       0.39.0\n",
      "PennyLane_Lightning_GPU   0.39.0\n",
      "PennyLane-Rigetti         0.39.0\n",
      "torch                     2.5.1\n",
      "torch_geometric           2.5.3\n",
      "torchaudio                2.5.1\n",
      "torcheval                 0.0.7\n",
      "torchsummary              1.5.1\n",
      "torchvision               0.20.1\n"
     ]
    }
   ],
   "source": [
    "!pip list | grep -e torch -e PennyLane -e networkx"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
